#### What
We will increase the difficulty and reset the last two layers of MNISTnet1
#### Why
The idea is to distance ourselves even more between the last unresetted embeddings of MNISTnet1 and the OPNet.



# Results
The training lasted 100 epochs. In the directory tensorboards, we can find the graphs corresponding to this training.

#### 1. Compare the original ImageOpNet and the new one on the PairMNIST validation set

| Metric    | Base               | New                |
| --------- | ------------------ | ------------------ |
| Test Loss | 43.28394317626953  | 43.25113296508789  |
| Test MAE  | 1.8919484615325928 | 1.8793364763259888 |

#### 2. Compare the original MNIST1 and the MNIST1 extracted from the new ImageOpNet on the MNIST validation set

| Metric    | Base Network        | New Network         |
| --------- | ------------------- | ------------------- |
| Test Loss | 0.16023200750350952 | 0.17085811495780945 |
| Test MAE  | 0.10831695795059204 | 0.1152801364660263  |

#### 3. Compare the ImageOpNet training of 1.1 and 1.2
in the following graphs blue is the experiment 1.2 and gray is the 1.1.

![[Pasted image 20240905203617.png]]![[Pasted image 20240905203700.png|600]]



# Discussion
Just like the experiment1.1, it seems that it was possible to connect the embeddings of the last convolutional layer of MNISTnet1 to the input layer of OPnet with quite success. There seems to be some difference between the results as seen in the previous graphs, but these might simply be due to chance. Either way, if we look at the validation MAE and validation loss, the difference is basically meaningless.