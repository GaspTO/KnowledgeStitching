# Overview
## What
Reset the parameters of the last linear layer of the first MNISTnet. Then train the ImageOperationNet, unfreezing only the linear layer's parameters.  
## Why
The idea here is to see if a simple linear layer can recover the number language. In theory, since it has learned it before, it should be able to learn it again, but as is experience, neural networks seem to need more parameters than what are strictly necessary in order to learn successfully.



# Results
The training lasted 100 epochs. In the directory tensorboards, we can find the graphs corresponding to this training.

#### 1. Compare the original ImageOpNet and the new one on the PairMNIST validation set

| Metric    | Base               | New               |
| --------- | ------------------ | ----------------- |
| Test Loss | 43.28394317626953  | 43.05772018432617 |
| Test MAE  | 1.8919484615325928 | 1.762453556060791 |

#### 2. Compare the original MNIST1 and the MNIST1 extracted from the new ImageOpNet on the MNIST validation set

| Metric    | Base Network        | New Network         |
| --------- | ------------------- | ------------------- |
| Test Loss | 0.16023200750350952 | 0.158871591091156   |
| Test MAE  | 0.10831695795059204 | 0.09800086915493011 |
A quick manual inspection also showed that it seems to have learned to decode numbers properly.


#### 3. Compare the ImageOpNet with reset on fc2 after only one training epoch

|              | Untrained         | After 1 Epoch       |
|--------------|-------------------|---------------------|
| Test Loss    | 3514.966796875    | 48.984657287597656  |
| Test MAE     | 44.74889373779297 | 3.03228759765625    |
	In fact, if we see the graph of the training we see that it the variability is bigger than the decrease:
![[Pasted image 20240905152642.png]]
Here's the same graph, but with maximum smoothing:
![[Pasted image 20240905153004.png]]


# Discussion
It is questionable whether or not we should aim for an equal score as the baseline, since the PairMNIST does not teach how to decode numbers - this is just a consequence. This task asks the MNISTnet to learn how to decode numbers as a requirement to solve PairMNIST.  

However, as we can see in the results, this wasn't the case -  we actually achieved a better score both in PairMNIST and MNIST after the reset. The score isn't insanely different, especially looking at the test loss, it is pretty much the same. So far, this makes us optimistic, since, as a consequence, the complex function signal of PairMNIST was able to reteach the last layer to learn MNIST. 
With this being said, this is not enough to prove that two networks can be stitched together since we knew it was possible in theory to stitch those two together again since this was its initial state, and the distance between the embeddings and the input of the OpNET was only one fc layer. In order to gain more confidence, we should increase the distance to two fc layers by freezing the two last layers of MNISTnet1.
